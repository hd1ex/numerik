\documentclass[a4paper]{article}
\usepackage[a4paper, margin=1.5cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{csquotes}
\usepackage{empheq} % Hervorhebung von Gleichungen, Teil vom Bündel »mathtools«
\usepackage[bookmarks]{hyperref}
\usepackage{amsmath}
\usepackage{setspace}

\onehalfspacing             % anderthalbfacher Zeilenabstand
%\doublespacing              % doppelter Zeilenabstand

\setlength{\parindent}{0pt} % Kein Einzug bei neuem Absatz

\newcommand{\set}[1]{\{ #1 \}}

\title{
    \textbf{Zusammenfassung Numerik}
    \\
    \large
    Dies ist eine Zusammenfassung des Skripts der KIT-Vorlesung
    \\
    \emph{Numerische Mathematik für die Fachrichtungen Informatik und
    Ingenieurwesen} von Daniel Weiß.
}
\author{Alexander Sommer}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Einführung}

\subsection{Normalisierte Gleitpunktzahlen}

$\mathrm{FL} := \set{ \pm B^e \sum\limits_{l = 1}^{L_m} a_l B^{-l} : e =
e_\mathrm{min} + \sum\limits_{l = 0}^{L_e - 1} c_l B^l, a_l, c_l \in \set{0,
\dots, B - 1}, a_1 \ne 0 } \cup \set{0} \subset \mathbb{Q}$.

\begin{itemize}
    \item $e_\mathrm{max} = e_\mathrm{min} + B^{L_e} - 1$
    \item $\max \mathrm{FL} = -\min \mathrm{FL} = B^{e_\mathrm{max}} (1 -
    B^{-L_m})$
    \item $\min \mathrm{FL}_+ = B^{e_\mathrm{min} - 1}$
\end{itemize}

\subsubsection{Darstellung reeller Zahlen im Computer}

Reele Zahl $x = \pm B^e \sum\limits_{l = 1}^\infty a_l B^{-l}$,
\hspace{1em}
$a_1 \ne 0, a_l \in \set{0, \dots, B - 1}$ 

wird zur Gleitpunktzahl

\begin{empheq}[left={\mathrm{fl}(x) := \pm B^e\empheqlbrace}]{alignat*=2}
    & \sum_{l = 1}^{L_m} a_l B^{-l},\ && \text{falls}\ a_{L_{m + 1}} <
    \frac{B}{2}\\
    & \sum_{l = 1}^{L_m} a_l B^{-l} + B^{-L_m},\ && \text{falls}\ a_{L_{m
    + 1}} \ge \frac{B}{2}\\
\end{empheq}

Für den relativen Fehler gilt:
$\frac{|x - \mathrm{fl}(x)|}{|x|} \le \frac{B^{(1 - L_m)}}{2} =: \mathrm{eps}$

was äquivalent ist zu $\mathrm{fl}(x) = x (1 - \epsilon)$
\hspace{1em}
mit $|\epsilon| \le \mathrm{eps}$.

\subsubsection{Gleitpunktrechnung}

\begin{itemize}
    \item Die Gleitpunktrealisierungen von $+, -, \cdot, /$ sind im Allgemeinen
    nicht assoziativ.
    \item eps ist \textbf{die kleinste Gleitpunktzahl, die auf 1 addiert eine
    andere Gleitpunktzahl ergibt}.
    \item Für $B = 2$ braucht bei normalisierten Gleitpunktzahlen die erste
    Nachkommastelle nicht gespeichert werden, da sie immer 1 ist. Beachte dazu
    auch: Die Zahl 0 wird in der Regel durch einen bestimmten Exponenten
    codiert. Wir können daher bei doppelter Genauigkeit die 52 Bit der Mantisse
    tatsächlich auf die Stellen 2 bis 53 verteilen und so eine höhere relative
    Genauigkeit erreichen. Mit diesem Trick lässt sich sogar die Zahl 1+eps als
    Maschinenzahl darstellen.
\end{itemize}

\subsection{Kondition eines Problems}

Die Frage nach der Kondition eines Problems lautet: Wie wirken sich Störungen
der Eingabedaten auf das Resultat unabhängig vom gewählten Algorithmus aus? Die
Kondition eines Problems ist ein Maß dafür, wie stark die Abhängigkeit der
Lösung von den Daten ist, also die unvermeidbare Fehlerverstärkung bei optimalem
Lösungsverfahren.

\begin{itemize}
    \item Die Summe zweier Zahlen mit gleichem Vorzeichen ist gut konditioniert.
    \item Bei der Differenz zweier fast gleich großer Zahlen kommt es zur
    \textbf{Auslöschung} von vertrauenswürdigen signifikanten Nachkommastellen
    der Mantisse und rundungsfehlerbehaftete Stellen bilden die vorderen
    Nachkommastellen der Differenz. Die Addition von x und y mit $x \approx -y$
    ist also sehr schlecht konditioniert und sollte vermieden werden.
\end{itemize}

\subsection{Stabilität eines Algorithmus}

Wir nennen ein numerisches Verfahren stabil, wenn Fehler innerhalb des
Verfahrens \enquote{nicht viel größer} sind als der Einfluss von Störungen der
Eingabedaten, der unvermeidbar durch die Kondition des Problems gegeben ist.
Dabei ist \enquote{nicht viel größer} relativ zu verstehen und abhängig von der
Komplexität des Algorithmus.

\section{Direkte Lösungsverfahren für lineare Gleichungssysteme}

\subsection{LR-Zerlegung}

$ A \in \mathbb{R}^{N \times N} $ regulär, $b \in \mathbb{R}^N$.
Suche $ x \in \mathbb{R}^{N} $, sodass $Ax=b$.
\\
Zerlegung $A = LR$ mit $L, R \in \mathbb{R}^{N \times N}$.

\begin{itemize}
    \item \enquote{Schlaues Gaußen} führt zu unterer und oberer Dreiecksmatrix
    (mit $\frac{1}{3}N^3$ Operationen).
    \item Ist $A$ \emph{nicht} positiv definit, so braucht die LR-Zerlegung
    zusätzlich noch Zeilenpermutationen $P \in \set{0, 1}^{N \times N} $, 
    also $PA = LR$.
    \item Löse LGS dann mit $Ly = Pb$ und $Rx = y$ durch Vorwärts- und
    Rückwärtssubstitution (je ca. $\frac{1}{2}N^2$ Operationen).
    \item $L$ hat auf der Diagonalen Einsen (normiert).
\end{itemize}

\begin{samepage}
\textbf{Berechnung} mittels \emph{Gauß-Elimination mit Spaltenpivotwahl}:
\\
Betrachte Spalten $i \in \set{1..N}$ von $A = a_{i, j}$ und wende an
\begin{enumerate}
    \item Betrachte Zeilen $i..N$.
    \item Tausche Zeile mit betragsmäßig größtem Element in der $i$-ten Spalte
    mit der $i$-ten Zeile (tausche sowohl in $A$, als auch in Darstellung von
    $P$).
    \item Eliminiere die $i$-ten Elemente der $z = i+1..N$-ten Zeile per
    Zeilenaddition mit Faktor $p_z \in \mathbb{R}$.
    \item Setze $l_{z, i} = -p_z$ (Beachte \emph{negierter} Wert!)
    \item Setze $r_{i, j} = a_{i, j}$ mit $(j \in {i..N})$
\end{enumerate}
\end{samepage}

\subsection{Cholesky-Zerlegung}

$ A \in \mathbb{R}^{N \times N} $ regulär und \emph{spd}, $b \in \mathbb{R}^N$.
Suche $ x \in \mathbb{R}^{N} $, sodass $Ax=b$.
\\
Zerlegung $A = LL^T$ mit $L \in \mathbb{R}^{N \times N}$.

\begin{itemize}
    \item Algorithmus führt zu unterer Dreiecksmatrix $L$
    (mit $\frac{1}{6}N^3$ Operationen).
    \item Löse LGS dann mit $Ly = Pb$ und $L^Tx = y$ durch Vorwärts- und
    Rückwärtssubstitution (je ca. $\frac{1}{2}N^2$ Operationen).
    \item Algorithmus schlägt fehl, wenn $A$ nicht \emph{spd} (also auch zum
    Testen auf \emph{spd}-heit sinnvoll)
\end{itemize}

\textbf{Berechnung} induktiv:
\\
$\bf{N = 1}$: $a = \sqrt{a}*\sqrt{a}$ für $a > 0$
\\
$\bf{N + 1}$: $A = 
\begin{pmatrix}
    A_{11}   & a_{21} \\
    a_{21}^T & a_{22}
\end{pmatrix}$,
$A_{11} = L_{11}L_{11}^T \in \mathbb{R}^{N \times N}$,
$a_{21} \in \mathbb{R}^{N}$,
$a_{22} \in \mathbb{R}$

\vspace{1em}
Löse durch Ausmultiplizieren.
\vspace{1em}

$A = $
$\begin{pmatrix}
    A_{11}   & a_{21} \\
    a_{21}^T & a_{22}
\end{pmatrix}$
=
$\begin{pmatrix}
    L_{11}   &  \\
    l_{21}^T & l_{22}
\end{pmatrix}$
$\begin{pmatrix}
    L_{11}^T & l_{21} \\
             & l_{22}
\end{pmatrix}$
=
$LL^T$

\vspace{1em}

Dadurch lassen sich die folgenden Gleichungen extrahieren:
\begin{enumerate}
    \item $L_{11}l_{21} = a_{21}$
    (Vorwärtssubstitution zur Berechnung von $l_{21}$).
    \item $a_{22} = l_{21}^T l_{21} + l_{22}^2$,
    also $l_{22} = \sqrt{a_{22} - l_{21}^T l_{21}}$.
\end{enumerate}

\subsection{QR-Zerlegung}

$ A \in \mathbb{R}^{M \times N} $, $M \ge N$, $Rang(A) = N$,
$b \in \mathbb{R}^M$.
Suche $ x \in \mathbb{R}^{N} $, sodass $Ax=b$.
\\
\\
Zerlegung $A = QR$ mit $Q \in \mathbb{R}^{M \times M}$, $QQ^T=I_M$
\\
\hspace{4em}
und $R = \begin{pmatrix}\bar{R}\\0\end{pmatrix} \in \mathbb{R}^{M \times N}$
$\bar{R} \in \mathbb{R}^{N \times N}$ obere Dreiecksmatrix. 

\begin{itemize}
    \item Berechne Matrizen $Q$ und $R$ durch
    Householder-Transformationen mit $A = QR$.
    \item Aufwand ist $\frac{2}{3} N^3$ im Fall $M \approx N$
    \item Löse LGS mit $Qc = b$ und $Rx = c$
    mit $c = Q^T b$ bzw. durch Rückwärtssubstitution.
    \item Algorithmus besonderes stabil, aber braucht ca. doppelt so viele
    Operationen wie die Gauß-Elimination.
\end{itemize}

Die Berechnung erfolgt mittels $N$ Householder-Transformationen
$Q_N \dots Q_2 Q_1A = R$ und $Q = Q_1 \dots Q_N$.
\\
$a^1 \in \mathbb{R}^M \setminus \set{0}$ ist die erste Spalte von
$A \in \mathbb{R}^{M \times N}$ und $Q_1 = I_M - 2w^1(w^1)^T$ die
Householder-Transformation mit $Q_1 a^1 = r_11 e^1 \in \mathbb{R}^M$
\dots (TODO).

\subsection{Kondition linearer Gleichungssysteme}

\subsubsection{Matrixnormen}

Varianten:
\begin{itemize}
    \item $||A|| = \sup\limits_{x \neq 0} \frac{||Ax||}{||x||}$
    (zugehörige Matrixnorm zu $||\cdot||$ auf $\mathbb{R}^N$)
    \item $||A||_1 = \max\limits_{m=1,...,N} \sum_{n=1}^N |a_{nm}|$
    (Spaltensummennorm)
    \item $||A||_2 = \sqrt{\textrm{größter EW von}~A^T A}$
    (Spektralnorm)
    \item $||A||_\infty = \max\limits_{n=1,...,N} \sum_{m=1}^N |a_{nm}|$
    (Zeilensummennorm)
\end{itemize}

Eigenschaften:
\begin{itemize}
    \item $||Ax|| \le ||A||\cdot||x||$
    \item $||I_n|| = 1$
    \item $||AB|| \le ||A||\cdot||B||$
\end{itemize}

\subsubsection{Konditionszahl}

$cond(A) = ||A|| \cdot ||A^{-1}||$

\vspace{1em}
Eigenschaften:
\begin{itemize}
    \item $1 = ||I_n|| = ||AA^{-1}|| \le ||A|| \cdot ||A^{-1}|| = cond(A)$
    \item $cond(A) = cond(\alpha A)$, $\alpha \in \mathbb{R} \setminus \{0\}$
    \item $cond(A) = \frac{\max_{||y|| = 1} ||Ay||}{\min_{||z|| = 1} ||Az||}$
\end{itemize}

\subsubsection{Lineare Ausgleichsrechnung}

\textbf{Lineares Ausgleichsproblem}

$A \in \mathbb{R}^{M \times N}$,
$M \ge N$,
$b \in \mathbb{R}^M$
\\
$||Ax-b||_2 = min!$

\vspace{2em}
\textbf{Satz 7}
\\
QR-Zerlegung von A, also $A^T A = $
$R = \begin{pmatrix}\bar{R}\\0\end{pmatrix}$
mit regulärem $\bar{R} \in \mathbb{R}^{N \times N}$
\\
Dann ist $x = \bar{R}^{-1} c$ ist Lösung vom linearen Ausgleichsproblem mit
$Q^T b = \begin{pmatrix}c\\d\end{pmatrix}$
\\
Für die Norm des Residuums $r = Ax - b$ gilt: $||r||_2 = ||d||_2$ 

\subsection{Singulärwertzerlegung}

Zu einer Matrix $A = \mathbb{R}^{M \times N}$ mit Rang R gibt es orthogonale
Matrizen $U \in \mathbb{R}^{M \times M}$, $V \in \mathbb{R}^{N \times N}$ mit
\\
$U^T A V = \Sigma = \begin{pmatrix}\Sigma_R & 0 \\ 0 & 0\end{pmatrix} \in
\mathbb{R}^{M \times N}$,
\hspace{2em}
$\Sigma_R = diag(\sigma_1, \dots, \sigma_R) \in \mathbb{R}^{R \times R}$
\\
wobei $\Sigma_R$ regulär und $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_R >
\sigma_{R + 1} = \dots = \sigma_p = 0$ mit $p = \min\{M, N\}$ heißen die
Singulärwerte der Matrix A.

\begin{enumerate}
    \renewcommand\labelenumi{\theenumi)}
    \item $A = U \Sigma V^T = U \sum\limits_{r = 1}^R \sigma_r e^r
    (e^r)^\intercal V^\intercal = \sum\limits_{r = 1}^R \sigma_r u^r
    (v^r)^\intercal$ (Singulärwertzerlegung von A).

    \item Für $M \ge N$ ist $x = \sum\limits_{r = 1}^R \frac{(u^r)^\intercal
    b}{\sigma_r} v^r$ die Lösung des linearen Ausgleichsproblems $||Ax - B||_2 =
    \min!$, $||x||_2 = \min!$

    \item Es gilt $||A||_2 = \sigma_1$, \hspace{2em} $||A||_F = \sqrt{\sigma_1^2
    + \dots + \sigma_R^2} := \sqrt{\textrm{Spur}(A^\intercal A)}$
    (Frobeniusnorm)
 
    \item Für $1 \le k \le R$ hat die Matrix $A_k := \sum\limits_{r = 1}^k
    \sigma_r u^r (v^r)^\intercal$ den Rang k und ist unter allen Matrizen mit
    Rang k bzgl. der Frobeniusnorm die beste Approximation von A
    (\emph{Bestapproximation}; gilt auch für die Spektralnorm).

    \item Neben den linearen Ausgleichsproblemen findet die
    Singulärwertzerlegung auch noch Anwendung in der Bildverarbeitung
    (Kompression, Filterung), dem Machine Learning (Textmining, Latent semantic
    indexing) und bei Recommender-Sytemen.
\end{enumerate}

\section{Iterative Verfahren für nichtlineare Gleichungssysteme}

\textbf{Problem:} Funktion $f: D \subset \mathbb{R}^N \to \mathbb{R}^N$, finde $x^*
\in D$ mit $f(x^*) = 0_N \in \mathbb{R}^N$

\textbf{Bemerkungen:}
\begin{itemize}
    \item Gleichungen können in die obere Form umgeformt werden bzw. sind in
    vielen Anwendungen Teilprobleme
    \item Gleichungen können keine, eine, mehrere oder unendlich viele Lösungen
    haben
    \item Bei linearen Gleichungen können \enquote{exakte} Lösungen angegeben
    werden, hier nur Näherungen
    \begin{itemize}
        \item Lineare Gleichung $\to$ Lösung mit Rundungsfehler
        \item Nichtlineare Gleichung $\to$ Lösung mit Rundungsfehler und
        Approximationsfehler
    \end{itemize}
\end{itemize}

\subsection{Newton-Verfahren}

$x^0$ ist in der Nähe der Lösung $x^*$. Wegen Taylor-Entwicklung gilt
$x^* \approx x^0 - f'(x^0)^{-1}f(x^0)$.
\\
Da man die Berechnung der inversen Jakobi-Matrix vermeiden möchte, löst man
alternativ das lineare Gleichungssystem $f'(x^0)d^0 = -f(x^0)$.
\\
Nun kann man $x^1 = x^0 + d^0$ setzen und das Verfahren beliebig oft
wiederholen.

\begin{samepage}
\textbf{Algorithmus:}
\begin{enumerate}
    \item Wähle Startwert $x^0 \in D$ und Toleranz $\epsilon$. Setze $k = 0$.
    \item\label{newton2} Löse $f'(x^k)d^k = -f(x^k)$ (LR-Zerlegung) und berechne
    $x^{k+1} = x^k + d^k$.
    \item Falls: $(||d^k|| < \epsilon)$: STOPP
          \\Erhöhe $k$ um 1 und gehe zu \ref{newton2}.
\end{enumerate}
\end{samepage}

\textbf{Anmerkung:} Als Abbruchkriterium sind auch noch andere Varianten
möglich. Bei $||f(x^k)|| < \epsilon$ sollte man vorsichtig sein, da durch
Ersetzung von $f$ mit $\lambda f$ das Kriterium beliebig manipuliert werden
kann.  

\subsection{Konvergenz des Newton-Verfahrens}

\textbf{Informell:} Das Newtonverfahren konvergiert lokal quadratisch für einen
Startwert $x^0 \in D$, wenn sich dieser in einer bestimmten Kugel um $x^*$ mit
dem Radius $\rho$ befindet und die Jakobi-Matrix ausgewertet an der Nullstelle
$x^*$ regulär ist (siehe Satz 8 im Kapitel 3.3 des Skripts). Die Kunst liegt nun
darin, einen solchen Startwert in dieser (unbekannten) Kugel zu finden.

\textbf{Bemerkungen:}
\begin{itemize}
    \item Für z. B. $f(x) = x^3$ gilt Satz 8 wegen $f'(x^*) = 0$ nicht, hier hat
    das Newtonverfahren nur lineare Konvergenz.
\end{itemize}

\subsection{Vereinfachtes Newton-Verfahren}

Beim vereinfachten Newtonverfahren wird die Ableitung durch eine konstante
Matrix $A \approx f'(x^0)$ ersetzt. 
\begin{itemize}
    \item Dadurch muss nur eine Auswertung und eine LR-Zerlegung berechnet
    werden
    \item Die Konvergenz ist nur noch \textbf{linear} 
    \item Entspricht Fixpunktoperation $F(x) = x - A^{-1}f(x)$
    \item Siehe Satz 9 im Kapitel 3.4 des Skripts
\end{itemize}

\subsection{Weitere Anwendungen des Newton-Verfahrens}

\begin{itemize}
    \item Globale Minimierung
    \item Nichtlineare Ausgleichsrechnung
\end{itemize}

\section{Interpolation und Approximation}

\textbf{Probleme:}
\begin{enumerate}
    \item Interpolation: Suche für Stützpunkte $(x_0, f_0), \dots, (x_N, f_N)$
    eine Funktion $p(x)$ mit $p(x_n) = f_n$.
    \item Approximation: Suche für eine gegebene Funktion $f: [a, b] \to
    \mathbb{R}$ eine möglichst einfach auszuwertende Funktion $p: [a, b] \to
    \mathbb{R}$, so dass $f - p$ \enquote*{klein} ist, z. B.
    \begin{enumerate}
        \item $\int_a^b(f(x) - p(x))^2dx = \min!$
        \item $\max\limits_{x \in [a, b]} |f(x) - p(x)| = \min!$
        \item Zusätzlich zu (a) oder (b): für endlich viele Punkte $f(x) = p(x)$
        (Interpolation).
    \end{enumerate}
\end{enumerate}

\subsection{Weierstraßscher Approximationssatz}

\textbf{Satz 10:} Gegeben eine stetige Funktion $f: [a, b] \to \mathbb{R}$.
\\
Dann existiert für jedes $\epsilon > 0$ ein $N \in \mathbb{N}$ und ein Polynom
$p: [a, b] \to \mathbb{R}$ vom Grad N mit $\max\limits_{x \in [a, b]} |f(x) -
p(x)| \le \epsilon$.

\subsection{Polynominterpolation}

\textbf{Eindeutigkeit:}
Ein Polynom vom Grad $\le N$ ist ist durch Vorgabe von $N + 1$ Punkten (an
paarweise verschiedenen Stellen) eindeutig bestimmt.

\textbf{Existenz:}
Lagrange-Polynome $L_n(x) = \prod\limits_{m=0, m \ne n}^N \frac{x - x_m}{x_n -
x_m}$ haben die Eigenschaft
\begin{empheq}[left={L_n(x_m)=\empheqlbrace}]{alignat*=2}
    & 0,\ && \text{falls}\ n \ne m \\
    & 1,\ && \text{falls}\ n = m
\end{empheq}

(der Zähler ist für den oberen Fall zuständig und der Nenner für die Normierung
des unteren Falls).
\\
Damit findet sich die Lösung $p(x) = \sum\limits_{n = 0}^N f_n L_n(x)$ für das
Interpolationsproblem.

\textbf{Satz 11:} Zu $N + 1$ Stützpunkten mit paarweise verschiedenen
Stützstellen existiert genau ein Interpolationspolynom vom Grad $\le N$.

\subsubsection{Kondition des Problems}

Stützpunkte $(x_n, f_n)$ und gestörte Stützpunkte $(x_n, \tilde{f}_n)$.
\\
Dann gilt $|p(x) - \tilde{p}(x)| \le \max\limits_{m = 0, \dots, N} |f_m -
\tilde{f}_m| \sum\limits_{n = 0}^N |L_n(x)|$.

\textbf{Definition 6:} Lebesgue-Konstante $\Lambda_N := \max\limits_{x \in [a, b]} \sum\limits_{n=
0}^N |L_n(x)|$

Damit gilt (Satz 12): $\max\limits_{x \in [a, b]} |p(x) - \tilde{p}(x)| \le
\Lambda_N \max\limits_{n = 0, \dots, N} |f_n - \tilde{f}_n|$

\subsubsection{Newton-Darstellung des Interpolationspolynoms}

\textbf{Satz 13:} Zu $N + 1$ Stützpunkten $(x_n, f_n)$, $n = 0, \dots, N$ mit
paarweise verschiedenen Stützstellen $x_n$ lässt sich das Interpolationspolynom
$P(x)$ vom Grad $\le N$ darstellen durch \\
$p(x) = f_{0, 0} + f_{0, 1} (x - x_0) + \dots + f_{0, N} (x - x_0) \cdot \ldots
\cdot (x - x_{N - 1})$
\\
wobei die dividierten Differenzen gegeben sind durch
\\
$f_{n, n} = f_n$, \hspace{7em} $n = 0, \ldots, N$
\\
$f_{n, k} = \frac{f_{n, k - 1} - f_{n + 1, k}}{x_n - x_k}$, \hspace{2.5em} $0
\le n < k \le N$.

\subsubsection{Das Restglied der Polynominterpolation}

TODO: Satz 14 (über den Interpolationsfehler).

\subsubsection{Tschebyscheff-Interpolation}

TODO

\subsection{Kubische Spline-Interpolation}

\textbf{Definition 7 (Informell):} Ein kubisch interpolierender Spline ist eine
$C^2$-Funktion $s: [a, b] \to \mathbb{R}$, welche eine Funktion durch kubische
Polynome zwischen den Stützpunkten interpoliert. Die Polynome sind an den
Stützpunkten \textbf{glatt} miteinander verbunden, also deren 1. und 2.
Ableitungen sind dort gleich.

\begin{samepage}
\textbf{Definition 8:} Ein solches s heißt im Fall
\begin{enumerate}
    \item $s'(a) = v_0$ und $s'(b) = v_N$ für vorgegebene $v_0, v_N$
    \textbf{eingespannter}
    \item $s''(a) = s''(b) = 0$ \textbf{natürlicher}
    \item $s'(a) = s'(b)$ und $s''(a) = s''(b)$ \textbf{periodischer}
    (ergibt nur für $y_0 = y_N$ Sinn)
\end{enumerate}
interpolierender kubischer Spline.
\end{samepage}

\subsubsection{Konstruktion}

Die Glattheitsforderung in Definition 7 resultiert in ein Gleichungssystem mit
$2N$ Unbekannten und $2N - 2$ Bedingungen. Mit den zusätzlichen 2 Forderungen
aus einer Option von Definition 8 kommen noch 2 weitere Bedingungen dazu.

Im Skript ist eine Herleitung eines solchen LGS für eingespannte kubische
Splines. 

\subsubsection{Kondition}

Der eingespannte \enquote{Lagrange}-Spline: $l_n(x_m) = \delta_{nm}$ und
$l_n'(a) = l_n'(b) = 0$.

\vspace{1em}

$|s(x) - \tilde{s}(x)| \le \Lambda_N \max\limits_{m = 0, \dots, N} |y_m -
\tilde{y}_m|$ mit $\Lambda_N := \max_{x \in [a, b]} \sum_{n = 0}^N |l_n(x)|$.

\vspace{1em}

\textbf{Satz 21:} Bei äquidistanter Unterteilung gilt $\Lambda_N \le 2$ für alle
$N \in \mathbb{N}$.

\section{Numerische Integration}

\textbf{Problem:} Berechne für gegebene Funktion $f: [a, b] \to \mathbb{R}$ das
Riemann-Integral $I(f) := \int_a^b f(x) dx$.

\textbf{Eigenschaften:}
\begin{enumerate}
    \item $I$ existiert für stückweise stetige Funktionen, hier werden nur
    \textbf{stetige} Funktionen betrachtet.
    \item I ist \textbf{additiv} bezüglich einer Zerlegung des
    Integrationsintervalls: $\int_a^b f(x) dx$ = $\int_a^c f(x) dx$ + $\int_c^b
    f(x) dx$.
    \item I ist \textbf{linear}:
    $I(\lambda f + \mu g) = \lambda I(f) + \mu I(g)$, mit $f, g \in
    C(\mathbb{R})$ und $\lambda, \mu \in \mathbb{R}$.
    \item $I$ ist \textbf{monoton}: Falls $f \ge g$ auf $[a, b]$, dann auch
    $\int_a^b f(x) dx \ge \int_a^b g(x) dx$.
    \\
    Zusätzlich folgt aus der Monotonie
    $|\int_a^b f(x) dx| \le \int_a^b |f(x)| dx$.
\end{enumerate}

\subsection{Kondition des Problems}

\textbf{Definition:} Norm $||f||_1 := \int_a^b |f(x)| dx = I(|f|)$.

\textbf{Satz 22 (relative Kondition)}: $f, \tilde{f} \in C([a, b], \mathbb{R})$.
Dann gilt: $\frac{|I(f) - I(\tilde{f})|}{|I(f)|} \le \mathrm{cond}_1 \frac{|f -
\tilde{f}|}{|f|}$, mit $\mathrm{cond}_1 := \frac{I(|f|)}{|I(f)|}$.

Das Problem ist schlecht konditioniert, wenn die Funktion stark (um die x-Achse)
oszilliert, sich die Flächen also gegenseitig auslöschen können.

\subsection{Quadraturformeln}

$\int_a^b f(x) dx \approx (b - a) \sum\limits_{K = 1}^s b_k f(a + c_k(b - a))$.

Quadraturformel ist also durch $(b_k, c_k)_{k = 1, \dots, s}$ bestimmt.

\begin{itemize}
    \item Eine Quadraturformel approximiert das Integral durch gewichtete Mittel
    der Stützpunkte.\\ $b_k$ sind dabei die Gewichte und $c_k$ bestimmen die
    Stützstellen.
    \item Die Funktion $f$ wird so gesehen durch ein Polynom approximiert,
    dessen (gewichtetes) Integral (exakt) durch eine Summe berechnet werden
    kann.
    \item $a +  x (b - a)$ bildet das Interval $[a, b]$ auf $[0, 1]$ ab.
    \item Es gilt $\sum\limits_{k = 1}^s b_k = 1$ (konstante Integranden).
\end{itemize}


\subsubsection{Ordnung einer Quadraturformel}

Die Approximationsgüte einer Quadraturformel wird durch die sogenannte Ordnung
beschrieben.

\textbf{Definition 9:} Eine Quadraturformel $(b_k, c_k)_{k = 1, \dots,s}$
besitzt die \textbf{Ordnung} p, falls sie exakte Lösungen für alle Polynome vom
Grad $\le p - 1$ liefert, wobei p maximal ist.

\textbf{Satz 23:} Eine Quadraturformel besitzt genau dann die Ordnung p, falls
$\frac{1}{q} = \sum\limits_{k = 1}^s b_k c_k^{q - 1}$ für alle $q = 1, \dots, p$
aber nicht mehr für $q = p + 1$ gilt.

\textbf{Satz 24 (Informell):} Satz 24 sagt aus, dass die Gewichte $b_k$ durch
die Formel von Satz 23 eindeutig bestimmt ist, wenn dir Ordnung min $s$ sein
soll.

\subsubsection{Symmetrische Quadraturformeln}

\textbf{Definition 10:} Sind symmetrisch, falls $c_k = 1 - c_{s + 1 -k}$, 
\hspace{1em}
$b_k = b_{s + 1 - k}$.

\textbf{Satz 25:} Die Ordnung einer symmetrischen Quadraturformel ist gerade.

\textbf{Satz 27:} Die maximale Ordnung einer Quadraturformel ist 2s.

\textbf{Satz 28 (Gauß-Quadraturformel):} $c_k = \frac{1}{2} (1 + \gamma_k)$
definiert eine eindeutige Quadraturformel der Ordnung 2s, wobei $\gamma_1,
\dots, \gamma_s$ die Nullstellen des Legendre-Polynoms vom Grad s sind. Die
Gewichte $b_k$ sind gemäß Satz 24 eindeutig bestimmt.

\subsection{Quadraturfehler}

$R(g)$ ordnet jeder Funktion $g: [0, 1] \to \mathbb{R}$ den Quadraturfehler zu.

\vspace{1em}

\textbf{Satz 29:} Die Quadraturformel habe die Ordnung p und $g : [0, 1] \to
\mathbb{R}$ sei q-mal stetig differenzierbar. Dann gilt $R(g) = \int_0^1 K_q(t)
g^{(q)} (t) dt$, falls $2 \le q \le p$ mit $K_q$ der Peano-Kern.

\section{Eigenwertprobleme}

\textbf{Problem:} Berechnung von Eigenwerten (EW) und Eigenvektoren (EV) einer
quadratischen Matrix A: $Av = \lambda v$.

Der Ansatz über die Nullstellenbestimmung des charakteristischen Polynoms ist
schlecht konditioniert und nicht effizient berechenbar.

\subsection{Kondition des Problems}

$A(\varepsilon) = A + \varepsilon C$ mit $c_{mn} \le a_{mn}$.

\textbf{Satz 30:} Sei $\lambda$ ein einfacher EW von A. Für hinreichend kleines
$\varepsilon$ gilt $\lambda(\varepsilon) = \lambda +  \varepsilon \frac{u^{T} C
v}{u^{T} v} + O(\varepsilon^2)$, wobei $\lambda(\varepsilon)$ EW von
$A(\varepsilon)$ ist, $Av = \lambda v$, $u^T A = \lambda u^T$ und $||v||_2 =
||u||_2 = 1$ gilt. 

\subsection{Vektoriteration}

\textbf{Satz 31:} Für eine symmetrische Matrix $A \in \mathbb{R}^{N \times N}$
sei ein einfacher Eigenwert $\lambda_1$ betragsmäßig größer als alle anderen.
Dann konvergiert die Folge von Vektoren $(y^k)_{k \in \mathbb{N}}$ definiert
durch die Vektoriteration $y_k = \frac{x^k}{||x^k||_2}$, $x^{k + 1} = A y^k$
gegen einen normierten Eigenvektor $v^1$ von $A$ zum Eigenwert $\lambda_1$,
falls $x^0$ nicht senkrecht auf dem Eigenraum $\langle\set{v^1}\rangle$ steht.

\subsubsection{Inverse Vektoriteration}

$\lambda_*$ ist Schätzung von $\lambda_n$, welches nahe 0 ist.

Dann lässt sich die Vektoriteration invers durchführen, indem man das
resultierende LGS löst: $y_k = \frac{x^k}{||x^k||_2}$, $A x^{k + 1} = y^k$.

Die Matrix an sich ist zwar schlecht konditioniert, aber die Berechnung der
Richtung des EV ist gut konditioniert.

\subsubsection{Spektrale Bisektion}

Hier wird ein Beispiel einer inversen Vektoriteration anhand eines
Graphenproblems gezeigt.

\subsection{QR-Algorithmus zur Eigenwertberechnung}

\textbf{QR-Algorithmus:}
\begin{enumerate}
    \item setze $A_0 = A$ und $k = 0$
    \item\label{qr2} zerlege $A_k = Q_k R_k$ ($QR$-Zerlegung von $A_k$)
    \item berechne $A_{k + 1} = R_k Q_k$, erhöhe $k$ um 1 und gehe zu \ref{qr2}.
\end{enumerate}

$A_k$ konvergiert dann zu einer Matrix $R$ mit den Eigenwerten $|\lambda_1| >
\dots > |\lambda_N|$ in der Diagonalen, falls diese reel sind.

\textbf{Bemerkungen:}
\begin{itemize}
    \item Der genannte Algorithmus ist in $O(N^3)$
    \\\textbf{Idee:} Transformiere $A$ in \emph{Hessebergform} $H$ und nutze die
    folgenden Eigenschaften:
    \begin{enumerate}
        \item Die Form bleibt für alle berechneten Matrizen erhalten.
        \item Die Transformation in $H$ und der Algorithmus damit sind beide in
        $O(N^2)$ durchführbar.
    \end{enumerate}
    \item Die Konvergenz gegeben durch $\frac{\lambda_2}{\lambda_1},
    \frac{\lambda_3}{\lambda_2}, \dots, \frac{\lambda_N}{\lambda_{N - 1}}$ ist
    sehr langsam (linear).
    \\\textbf{Idee:} Benutze zusätzlich zur vorgeschlagenen Transformation einen
    \emph{Shift} $\mu_k$:
    \begin{enumerate}
        \item setze $H_0 = H$ und $k = 0$
        \item\label{qr-shift-2} zerlege $H_k - \mu_k I_N = Q_k R_k$
        \item berechne $H_{k + 1} = R_k Q_k + \mu_k I_N$, erhöhe $k$ um 1 und
        gehe zu \ref{qr-shift-2}.
        \begin{itemize}
            \item \textbf{Abbruch:} TODO
            \item \textbf{Konvergenz:}
            \begin{itemize}
                \item Wenn $\delta^2 \ll a^2$, dann $O(\delta^2)$ (quadratisch)
                \item Wenn $H = H^T$, dann $O(\delta^3)$ (kubisch) 
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{itemize}

\textbf{Satz 32:} Eine Matrix $A \in \mathbb{R}^{N \times N}$ kann durch $N - 2$
Householder-Transformationen auf Hessebergform gebracht werden:
$Q^T A Q = H =
\begin{pmatrix}
    * & * & \dots & *\\
    * & * & \dots & *\\
      & \ddots & \ddots & \vdots\\
      &   & *     & *
\end{pmatrix}
$
mit $Q = Q_2 \dots Q_{N - 1}$, wobei $Q_n$ für $n = 2, \dots, N - 1$
Householder-Transformationen sind. Ist $A$ symmetrisch, so ist $H$ eine
Tridiagonalmatrix.

\textbf{Satz 33} und \textbf{Satz 34} formalisieren die oben genannten Ideen und
Eigenschaften: \textbf{Satz 33} beschreibt die Formerhaltung der Hessebergform
im Algorithmus und \textbf{Satz 34} macht Aussagen über die schnelle Konvergenz,
wenn ein Shift benutzt wird.

\textbf{Bemerkung:} Der QR-Algorithmus wird unter anderem bei der
Singulärwertberechnung verwendet.

\end{document}
